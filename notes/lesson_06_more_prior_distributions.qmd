---
title: "Lesson 6: Power Priors and Prior Effective Sample Size"
author: "Gabriel Odom, Kazi Tanvir Hasan, and Ning Sun"
date: "2022-10-26"
format: html
---


# Power Priors

The power prior has emerged as a useful informative prior for the incorporation of historical data in a Bayesian analysis.

Suppose we have historical data from a similar previous study, with sample size $n_0$. Further, let $Ï€_0(\theta)$ denote the prior distribution for $\theta$. Let the data from the current study be denoted as $D$, with sample size $n$. 

Given the parameter $a_0$, we define the power prior distribution of $\theta$ for the current study as $\pi (\theta|D_0, a_0) \propto L(\theta|D_0)^{a_0} \pi_0(\theta)$, where $a_0$ weights the historical data relative to the likelihood of the current study, and thus the parameter a0 controls the influence of the historical data on $L(\theta|D)$. 

Such control is important in cases where there is heterogeneity between the previous and current studies, or when the sample sizes of the two studies are quite different. 

it is unnatural in most applications to weight the historical data more than the current data. Thus, it is scientifically more sound to restrict the range of $a_0$ to be between 0 and 1.

# Prior Effective Sample Size

## Basic Example (Beta-Binomial)

The ideas herein are largely inspired by this paper from Peter Thall: <https://web.ma.utexas.edu/users/pmueller/pap/MTM08.pdf>. Also consider <https://doi.org/10.1111/biom.13124>.


### Theoretical Framework
We know that the conjugate prior for the Binomial distribution is Beta.
That is, 

$$
\begin{align*}
\pi(p|n, k, \alpha, \beta)
&=
  \frac{
    \mathcal{L}(p|n, k) \times \pi(p)
  }{
    \int_0^1 \mathcal{L}(p|n, k) \times \pi(p) dp
  } \\
&=
  \frac{
    {n\choose k} p^{k}(1 - p)^{n - k} \times
    \frac{ \Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta) } p^{\alpha - 1} (1 - p)^{\beta - 1}
  }{
    {n\choose k} \frac{ \Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta) } \frac{\Gamma(k + \alpha)\Gamma(n - k + \beta)}{\Gamma(\alpha + \beta + n)}
  } \\
&=
  \frac{\Gamma(\alpha + \beta + n)}{\Gamma(k + \alpha)\Gamma(n - k + \beta)} p^{k + \alpha - 1} (1 - p)^{n - k + \beta - 1}.
\end{align*}
$$

This means that the parameters of the prior distribution, $\alpha$ and $\beta$, are effectively adding "samples" to the new distribution.


### Visualizing this Effect
Let's pretend we have 10 samples with 6 successes. We define the Beta-Binomial posterior below:
```{r}
n <- 10
k <- 6
p <- seq(0.0001, 0.9999, by = 0.001)

posteriorBBinom <- function(p, alpha, beta) {
	const <- gamma(alpha + beta + n) / (gamma(alpha + k) * gamma(n - k + beta))
	kern <- p ^ (k + alpha - 1) * (1 - p) ^ (n - k + beta - 1)
	const * kern
}
```


#### Beta(2, 2)
If our prior was a Beta(2, 2), then the posterior distribution of $p$ would look like:
```{r}
plot(
	x = p,
	y = posteriorBBinom(p, alpha = 2, beta = 2)
)
```

Note that the posterior mode (maximum value) is
```{r}
bestP_22 <- which.max(posteriorBBinom(p, alpha = 2, beta = 2))
p[bestP_22]
```

Why isn't it 0.6? Because of our *prior information*. Our prior information was that $\alpha = \beta = 2$, a symmetric distribution with mean $\alpha / (\alpha + \beta) = 0.5$. This means that the posterior mean is analogous to a weighted average of the prior information and the current data. Using this posterior, the mean is $(k + \alpha) / (k + \alpha + n - k + \beta) = 8 / (8 + 6) = 4/7 \approx 0.57$.


# Assignment

For the Beta-Binomial example above, vary the hyperparameters for the Beta Distribution and comment on the results.

