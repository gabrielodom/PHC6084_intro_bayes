---
title: "Lesson 4B: Continuous Distributions"
subtitle: "The Normal Distribution"
author: "Gabriel Odom, Kazi Tanvir Hasan, and Ning Sun"
date: "2022-10-05"
format: html
---


# Introduction


The normal distribution on $(-\infty, \infty)$ is defined as:

$$ f(x) = \frac{1}{\sqrt{2 \pi \sigma^2}} exp[\, - \frac{1}{2 \sigma^2} (x - \mu)^2 ]\,$$


# Moment Generating Function


Normal distributed X can be transformed as $X = \mu + \sigma * Z$, Z follows standard normal distribution.
$$
\begin{align*}
M_z(t) 
&= E(e^{tz}) \\
&= \int_S \frac{1}{\sqrt{2\pi}}  e^{tz} \times e^{- \frac{1}{2}z^2} dz \\
&= \frac{1}{\sqrt{2\pi}} \int_S e^{- \frac{1}{2}(z^2 - 2tz + t^2 - t^2)} dz\\
&= \frac{1}{\sqrt{2\pi}} \int_S e^{- \frac{1}{2}(z-t)^2} \times e^{\frac{1}{2} t^2} dz .
\end{align*}
$$

Let $y = z - t$, then $dy = dz$

$$
\begin{align*}
M_z(t) 
&= \frac{1}{\sqrt{2\pi}} \int_S e^{- \frac{1}{2} y^2} \times e^{\frac{1}{2} t^2} dy \\
&= e^{\frac{1}{2} t^2} \int_S \frac{1}{\sqrt{2\pi}} e^{- \frac{1}{2} y^2} dy \\
&= e^{\frac{1}{2} t^2}
\end{align*}
$$

With this fact, we can have the MGF of x easily.

$$
\begin{align*}
M_x(t) 
&= E(e^{(\mu + \sigma z) t}) \\
&= E(e^{\mu t} \times e^{(\sigma z) t}) \\
&= e^{\mu t} E(e^{(\sigma t) z}) \\
&= e^{\mu t} M_z(\sigma t) \\
&= e^{\mu t} \times e^{\frac{\sigma^2 t^2}{2}}
\end{align*}
$$


# First Moment


$$
\begin{align*}
E(x) 
&= E(\mu + \sigma z) \\
&= E(\mu) + \sigma E(z) \\
&= \mu + \sigma \times 0 \\
&= \mu
\end{align*}
$$

Or, use the moment generating function, we can have:

$$
\begin{align*}
E(x) 
&= M_x^{'}(0) \\
&= e^{\mu t} \times e^{\frac{\sigma^2 t^2}{2}} \times (\mu + \sigma^2 t) \\
&= e^0 \times e^0 \times \mu \\
&= \mu
\end{align*}
$$


# Second Moment and Variance


$$
\begin{align*}
E(x^2) 
&= M_x^{''}(0) \\
&= (e^{\mu t} \times e^{\frac{\sigma^2 t^2}{2}} \times (\mu + \sigma^2 t) )^{'} \\
&= e^{\mu t} \times e^{\frac{\sigma^2 t^2}{2}} \times \sigma^2 + e^{\mu t} \times e^{\frac{\sigma^2 t^2}{2}} \times (\mu + \sigma^2 t) \times (\mu + \sigma^2 t) \\
&= e^0 \times e^0 \times \sigma^2 + e^0 \times e^0 \times \mu \times \mu \\
&= \sigma^2 + \mu^2
\end{align*}
$$

Then we can calculate the variance. 

$$
\begin{align*}
Var(x) 
&= E(x^2) - [E(x)]^2 \\
&= \sigma^2 + \mu^2 - \mu^2 \\
&= \sigma^2
\end{align*}
$$


# Likelihood and Log-Likelihood Functions


$$
\begin{align*}
\mathcal{L}(\theta | X) 
&= \prod_i \frac{1}{\sqrt{2 \pi \sigma^2}} exp[\, - \frac{1}{2 \sigma^2} (x_i - \mu)^2 ]\,
\end{align*}
$$


# Maximum Likelihood Estimators


let $v = \sigma^2$, then

$$\ell(\theta|X) = \frac{n}{2} ln(\frac{1}{2 \pi}) - \frac{n}{2} ln(v) - \frac{1}{2v}\sum_{i = 1}^n(x_i - \mu)^2 $$

Set the partial derivatives equal to 0:

$$
\frac{\partial \ell}{\partial \mu} = - \frac{1}{v} \sum_1^n (x_i - \mu) = 0
$$
$$
\frac{\partial \ell}{\partial v} = - \frac{n}{2v} + \frac{1}{2v^2} \sum_1^n (x_i - \mu)^2 = 0
$$

Therefore, we have
$$\hat{\mu} = \frac{\sum_1^n x_i}{n} = \bar{x}$$
$$\hat{v} = \frac{\sum_1^n (x_i - \bar{x})^2}{n} $$

## Analytic Solution

```{r, warning=FALSE, message=FALSE}
library(tidyverse)

# sample
set.seed(2022)
sample_normal <- rnorm(10000, 0, 1)

# MLE
muHat <- mean(sample_normal)
muHat
sigma2Hat <- sum((sample_normal - muHat)^2)/10000
sigma2Hat
```
For this seed, the analytical MLE for mu is -0.01 and for variance is 0.99, via the simulation

## Numeric Solution

```{r}

# log-likelihood
lnormal <- function(X, mu, sigma2, n) {
  n / 2 * log(1 / (2 * pi)) - n / 2 * log(sigma2) - 1 / (2 * sigma2) * sum((X - mu)^2)
}

# Range of parameters
muDomain <- rep(seq(from = -5, to = 5, by = 0.01), each = 500)
sigma2Domain <- rep(seq(from = 0.01, to = 5, by = 0.01), 1001)

# likelihood

likelihood_nums <- map2_dbl(
	.x = muDomain,
	.y = sigma2Domain,
	.f = ~lnormal(X = sample_normal , mu = .x, sigma2 = .y, n = length(sample_normal))
)

results <- tibble(mu = muDomain, sigma2 = sigma2Domain, Likelihood = likelihood_nums) 


# what is the maximum likelihood for mu and sigma^2?

results %>% 
  arrange(desc(Likelihood)) %>% 
	head(10)
```  

For this seed, the numerical mle of mu is -0.01 and of sigma^2 is 0.99, via simulation. 


# Assignments


Replicate these steps for the Gamma Distribution, and derive the Inverse Gamma Distribution.
