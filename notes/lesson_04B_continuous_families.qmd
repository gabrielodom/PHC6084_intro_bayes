---
title: "Lesson 4B: Continuous Distributions"
subtitle: "The Normal Distribution"
author: "Gabriel Odom, Kazi Tanvir Hasan, and Ning Sun"
date: "2022-10-05"
format: html
---



# Introduction

The Normal Distribution, denoted by $\text{N}(\mu, \sigma^2)$ has PDF
$f(x|\mu, \sigma^2) \equiv \frac{1}{\sigma \sqrt{2\pi}}e^{-\frac{1}{2\sigma^2}(x-\mu)^2}$  for $\mu \in (-\infty, \infty)$, $\sigma^2 \in (-0, \infty)$ and $x \in \mathbb{R}$.


The normal distribution on $(-\infty, \infty)$ is defined as:

<<<<<<< HEAD
$$ f(x) = \frac{1}{\sqrt{2 \pi \sigma^2}} exp[\, - \frac{1}{2 \sigma^2} (x - \mu)^2 ]\,$$
=======
# First Moment

\begin{align*}
E[X] &= \int_{-\infty}^{\infty} x \frac{1}{\sigma \sqrt{2\pi}}e^{-\frac{1}{2\sigma^2}(x-\mu)^2} dx\\
&= \frac{1}{\sigma \sqrt{2\pi}} 
\int_{-\infty}^{\infty} x e^{-\frac{1}{2\sigma^2}(x-\mu)^2} dx\\
\end{align*}

Let, 
\begin{align*}
z &= \frac{x-\mu}{\sigma} \\ 
\implies x &= \sigma z + \mu \\ 
\implies dx = \sigma dz
\end{align*}

\begin{align*}
\therefore E[X] &= \frac{1}{\sigma \sqrt{2\pi}} \int_{-\infty}^{\infty} (\sigma z + \mu)e^{-\frac{\sigma z + \mu - \mu}{2\sigma^2}} dz\\
&= \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} (\sigma z + \mu) e^{-\frac{z^2}{2}}dz\\
&= \frac{1}{\sqrt{2\pi}}\left [\int_{-\infty}^{\infty} \sigma z e^{-\frac{z^2}{2}} + \int_{-\infty}^{\infty} \mu z e^{-\frac{z^2}{2}}dz \right]
\end{align*}

Now, $\int_{-\infty}^{\infty} z e^{-\frac{z^2}{2}} dz= 0; \quad$ 
Since, this is an odd function.

and, $\int_{-\infty}^{\infty} e^{-\frac{z^2}{2}} dz= 2\int_{0}^{\infty} e^{-\frac{z^2}{2}}dz; \quad$ Since, this is an even function.

So, \begin{align*}
E[X] &= \frac{\mu}{\sqrt 2\pi}2 \int_{0}^{\infty} e^{-\frac{z^2}{2}}dz\\
&= \frac{\mu}{\sqrt \pi} \sqrt \pi; \quad[\text{Since,} \int_{0}^{\infty} e^{-\frac{x^2}{2}}dx = \sqrt \pi]\\
\therefore E[X] &= \mu
\end{align*}



# Second Moment and Variance

\begin{align*}
Var[X] &= \int_{-\infty}^{\infty} (x-\mu)^2 \frac{1}{\sigma \sqrt{2\pi}}e^{-\frac{1}{2\sigma^2}(x-\mu)^2} dx\\
&= \frac{1}{\sigma \sqrt{2\pi}} 
\int_{-\infty}^{\infty} (x-\mu)^2 e^{-\frac{1}{2\sigma^2}(x-\mu)^2}dx \\
\end{align*}

Let, 
\begin{align*}
z &= \frac{x-\mu}{\sigma} \\ 
\implies x &= \sigma z + \mu \\ 
\implies dx = \sigma dz
\end{align*}

\begin{align*}
\therefore Var[X] &= \frac{1}{\sigma \sqrt{2\pi}} \int_{-\infty}^{\infty} (\sigma z)^2 e^{-\frac{\sigma z + \mu - \mu}{2\sigma^2}}dz\\
&= \frac{\sigma^2}{\sqrt{2\pi}} \int_{-\infty}^{\infty}  z^2  e^{-\frac{z^2}{2}}dz\\
&= \frac{\sigma^2}{\sqrt{2\pi}} 2\int_{0}^{\infty}  z^2  e^{-\frac{z^2}{2}}dz;
\quad \text{Since, this is an even function}\\
\end{align*}

Again, let-
\begin{align*}
z^2/2 &= t \\ 
\implies z^2 &= 2t \\ 
\implies z &= \sqrt{2t} \\ 
\implies dz =  \frac{2dt}{2z} = \frac{dt}{\sqrt{2t}}
\end{align*}

Then,
\begin{align*}
\therefore Var[X] &= \frac{2\sigma^2}{\sqrt{2\pi}} \int_{0}^{\infty}  2t  e^{-t}\frac{dt}{\sqrt{2t}}\\
&= \frac{2\sigma^2}{\sqrt{\pi}} \int_{0}^{\infty} t^{1/2}  e^{-t} dt\\
&= \frac{2\sigma^2}{\sqrt{\pi}} \int_{0}^{\infty} e^{-t} t^{\frac{3}{2}-1} dt\\
&= \frac{2\sigma^2}{\sqrt{\pi}} \Gamma{\left(\frac{3}{2}\right)}; \quad \left[\text{Since,} \Gamma{(n)}=  \int_{0}^{\infty} e^{-x} x^{n-1} dx \right]\\
&= \frac{2\sigma^2}{\sqrt{\pi}} \frac{1}{2}\Gamma{\left(\frac{1}{2}\right)}\\
&= \frac{2\sigma^2}{\sqrt{\pi}} \frac{1}{2}\sqrt{\pi}\\
\therefore Var[X] &= \sigma^2
\end{align*}

Then, the second moment- 
$E[X^2]= Var[X] - {E[X]}^2 = \sigma^2 - \mu^2$

>>>>>>> c56ca554d00bc4ab018f84386d75fb034a24da7e


# Moment Generating Function

\begin{align*}
M_X{(t)} = E[e^{tx}] &= \int_{-\infty}^{\infty} e^{tx} \frac{1}{\sigma \sqrt{2\pi}}e^{-\frac{1}{2\sigma^2}(x-\mu)^2} dx\\
 &= \frac{1}{\sigma\sqrt{2\pi}}\int_{-\infty}^{\infty} e^{tx} e^{-\frac{1}{2\sigma^2}(x-\mu)^2} dx
\end{align*}

Let, 
\begin{align*}
z &= \frac{x-\mu}{\sigma} \\ 
\implies x &= \sigma z + \mu \\ 
\implies dx = \sigma dz
\end{align*}

Then,
\begin{align*}
M_X{(t)} &= \frac{1}{\sigma\sqrt{2\pi}}\int_{-\infty}^{\infty} e^{\mu + \sigma z}t e^{-\frac{z^2}{2}(x-\mu)^2} \sigma dz\\
&=\frac{e^{\mu t}}{\sqrt{2\pi}}\int_{-\infty}^{\infty} e^{\sigma zt} e^{-\frac{z^2}{2}(x-\mu)^2} \sigma dz\\
&=\frac{e^{\mu t}}{\sqrt{2\pi}}\int_{-\infty}^{\infty} e^{z^2-2\sigma zt} \sigma dz\\
&=\frac{e^{\mu t}}{\sqrt{2\pi}}\int_{-\infty}^{\infty} e^{\frac{1}{2}{(z-\sigma t)^2- \sigma^2 t^2}} \sigma dz\\
&=\frac{e^{\mu t}}{\sqrt{2\pi}} e^{\frac{\sigma^2t^2}{2}}\int_{-\infty}^{\infty} e^{\frac{1}{2}{(z-\sigma t)^2}} \sigma dz\\
&=e^{\mu t} e^{\frac{\sigma^2t^2}{2}}\int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}} e^{\frac{1}{2}{(z-\sigma t)^2}} \sigma dz\\
\therefore M_X{(t)} &= e^{\mu t} e^{\frac{\sigma^2t^2}{2}}; \quad [\text{Since,} z \sim N(\sigma t, 1)]
\end{align*}


Normal distributed X can be transformed as $X = \mu + \sigma * Z$, Z follows standard normal distribution.
$$
\begin{align*}
M_z(t) 
&= E(e^{tz}) \\
&= \int_S \frac{1}{\sqrt{2\pi}}  e^{tz} \times e^{- \frac{1}{2}z^2} dz \\
&= \frac{1}{\sqrt{2\pi}} \int_S e^{- \frac{1}{2}(z^2 - 2tz + t^2 - t^2)} dz\\
&= \frac{1}{\sqrt{2\pi}} \int_S e^{- \frac{1}{2}(z-t)^2} \times e^{\frac{1}{2} t^2} dz .
\end{align*}
$$

Let $y = z - t$, then $dy = dz$

$$
\begin{align*}
M_z(t) 
&= \frac{1}{\sqrt{2\pi}} \int_S e^{- \frac{1}{2} y^2} \times e^{\frac{1}{2} t^2} dy \\
&= e^{\frac{1}{2} t^2} \int_S \frac{1}{\sqrt{2\pi}} e^{- \frac{1}{2} y^2} dy \\
&= e^{\frac{1}{2} t^2}
\end{align*}
$$

With this fact, we can have the MGF of x easily.

$$
\begin{align*}
M_x(t) 
&= E(e^{(\mu + \sigma z) t}) \\
&= E(e^{\mu t} \times e^{(\sigma z) t}) \\
&= e^{\mu t} E(e^{(\sigma t) z}) \\
&= e^{\mu t} M_z(\sigma t) \\
&= e^{\mu t} \times e^{\frac{\sigma^2 t^2}{2}}
\end{align*}
$$


# First Moment


$$
\begin{align*}
E(x) 
&= E(\mu + \sigma z) \\
&= E(\mu) + \sigma E(z) \\
&= \mu + \sigma \times 0 \\
&= \mu
\end{align*}
$$

Or, use the moment generating function, we can have:

$$
\begin{align*}
E(x) 
&= M_x^{'}(0) \\
&= e^{\mu t} \times e^{\frac{\sigma^2 t^2}{2}} \times (\mu + \sigma^2 t) \\
&= e^0 \times e^0 \times \mu \\
&= \mu
\end{align*}
$$


# Second Moment and Variance


$$
\begin{align*}
E(x^2) 
&= M_x^{''}(0) \\
&= (e^{\mu t} \times e^{\frac{\sigma^2 t^2}{2}} \times (\mu + \sigma^2 t) )^{'} \\
&= e^{\mu t} \times e^{\frac{\sigma^2 t^2}{2}} \times \sigma^2 + e^{\mu t} \times e^{\frac{\sigma^2 t^2}{2}} \times (\mu + \sigma^2 t) \times (\mu + \sigma^2 t) \\
&= e^0 \times e^0 \times \sigma^2 + e^0 \times e^0 \times \mu \times \mu \\
&= \sigma^2 + \mu^2
\end{align*}
$$

Then we can calculate the variance. 

$$
\begin{align*}
Var(x) 
&= E(x^2) - [E(x)]^2 \\
&= \sigma^2 + \mu^2 - \mu^2 \\
&= \sigma^2
\end{align*}
$$


# Likelihood and Log-Likelihood Functions

The likelihood function of $\mu$ and $\sigma^2$ is-

$L(\mu, \sigma^2 | x) = \prod_i f(x|\mu,\sigma^2) = \left(\frac{1}{\sigma \sqrt2\pi} \right)^n e^{-\frac{\sum_i(x_i-\mu)}{2\sigma^2}} $

and, the log-likelihood function is-
\begin{align*}
l(\mu, \sigma^2 | x) &= log(L(\mu, \sigma^2 | x))\\
&= nlog(\frac{1}{\sqrt 2\pi})-\frac{n}{2}log(\sigma^2)-\frac{\sum_i(x_i-\mu)}{2\sigma^2}
\end{align*}


$$
\begin{align*}
\mathcal{L}(\theta | X) 
&= \prod_i \frac{1}{\sqrt{2 \pi \sigma^2}} exp[\, - \frac{1}{2 \sigma^2} (x_i - \mu)^2 ]\,
\end{align*}
$$


# Maximum Likelihood Estimators

The Maximum Likelihood Estimator (MLE) for $\mu = \bar{x}$ and $\sigma^2 = \frac{1}{n}\sum_i(x_i - \mu)^2$.


let $v = \sigma^2$, then

$$\ell(\theta|X) = \frac{n}{2} ln(\frac{1}{2 \pi}) - \frac{n}{2} ln(v) - \frac{1}{2v}\sum_{i = 1}^n(x_i - \mu)^2 $$

Set the partial derivatives equal to 0:

$$
\frac{\partial \ell}{\partial \mu} = - \frac{1}{v} \sum_1^n (x_i - \mu) = 0
$$
$$
\frac{\partial \ell}{\partial v} = - \frac{n}{2v} + \frac{1}{2v^2} \sum_1^n (x_i - \mu)^2 = 0
$$

Therefore, we have
$$\hat{\mu} = \frac{\sum_1^n x_i}{n} = \bar{x}$$
$$\hat{v} = \frac{\sum_1^n (x_i - \bar{x})^2}{n} $$

## Analytic Solution

<<<<<<< HEAD
```{r, warning=FALSE, message=FALSE}
library(tidyverse)

# sample
set.seed(2022)
sample_normal <- rnorm(10000, 0, 1)

# MLE
muHat <- mean(sample_normal)
muHat
sigma2Hat <- sum((sample_normal - muHat)^2)/10000
sigma2Hat
```
For this seed, the analytical MLE for mu is -0.01 and for variance is 0.99, via the simulation
=======
For $\mu$,
\begin{align*}
\frac{\partial f}{\partial \mu} &= 0\\
\implies 0 - 0 - \frac{1}{2\sigma^2} \sum_i -2(x_i - \mu) &= 0\\
\implies \frac{1}{\sigma^2} \sum_i -(x_i - \mu) &= 0\\
\implies \sum_i {x_i} - n\mu &= 0\\
\therefore \hat{\mu}_{mle} = \frac{1}{n} \sum_i {x_i} = \bar{x}
\end{align*}

For $\sigma^2$,
\begin{align*}
\frac{\partial f}{\partial \sigma^2} &= 0\\
\implies 0 - \frac{n}{2} \left( \frac{1}{\sigma^2} \right) 2\sigma - \frac{1}{2}\frac{2}{\sigma^3} \sum_i(x_i - \mu)^2 &= 0\\
\implies \frac{n}{\sigma}- \frac{\sum_i(x_i - \mu)^2}{\sigma^3} &= 0\\
\implies n\sigma^2 &= \sum_i(x_i - \mu)^2\\
\therefore \hat{\sigma^2}_{mle} &= \frac{1}{n}\sum_i(x_i - \mu)^2
\end{align*}

>>>>>>> c56ca554d00bc4ab018f84386d75fb034a24da7e

## Numeric Solution

```{r}
<<<<<<< HEAD

# log-likelihood
lnormal <- function(X, mu, sigma2, n) {
  n / 2 * log(1 / (2 * pi)) - n / 2 * log(sigma2) - 1 / (2 * sigma2) * sum((X - mu)^2)
}

# Range of parameters
muDomain <- rep(seq(from = -5, to = 5, by = 0.01), each = 500)
sigma2Domain <- rep(seq(from = 0.01, to = 5, by = 0.01), 1001)

# likelihood

likelihood_nums <- map2_dbl(
	.x = muDomain,
	.y = sigma2Domain,
	.f = ~lnormal(X = sample_normal , mu = .x, sigma2 = .y, n = length(sample_normal))
)

results <- tibble(mu = muDomain, sigma2 = sigma2Domain, Likelihood = likelihood_nums) 


# what is the maximum likelihood for mu and sigma^2?

results %>% 
  arrange(desc(Likelihood)) %>% 
	head(10)
```  

For this seed, the numerical mle of mu is -0.01 and of sigma^2 is 0.99, via simulation. 
=======
library(tidyverse)
set.seed(20220919)
sampleNorm <- rnorm(n = 10000, mean = 5, sd = 2)
```
>>>>>>> c56ca554d00bc4ab018f84386d75fb034a24da7e

What is the log-likelihood of $p$?

```{r}
lNorm <- function(mu, sigma, k) {
  (length(k) * log(1/(sqrt(2 * pi)))) 
  - ((length(k)/2) * log(sigma^2)) 
  - ((sum(k - mu)^2) / (2 * sigma^2))
}

# Range of mu's and sigma's
muDomain <- seq(from = -10000, to = 10000, by = 1) 
sigmaDomain <- seq(from = 1, to = 21, by = 0.001) 

# log-Likelihood is the log-sum of the PDFs for all values of k
likelihoods_num <- map2_dbl(
	.x = muDomain,
	.y = sigmaDomain,
	.f = ~{
		lNorm(mu = .x, sigma = .y, k = sampleNorm)
	}
)
```

Let's plot it:

```{r, warning=FALSE}
resNorm_df <- tibble(
  X = muDomain, Y = sigmaDomain, Likelihood = likelihoods_num
)
ggplot(data = resNorm_df) +
	aes(x = X, y = Likelihood) + 
	geom_point()

ggplot(data = resNorm_df) +
	aes(x = Y, y = Likelihood) + 
	geom_point()
	
```

What is the maximum likelihood for $\mu$ and $\sigma$?

```{r}
resNorm_df %>% 
	arrange(desc(Likelihood))
```

For this seed, the MLE of $\mu$ is 5 and $\sigma$ is 10.005 via simulation. However, the closed form solution of these MLEs are $\frac{1}{n} \sum_i k_i = \bar{k}$ and $\frac{1}{n} \sum_i (k_i-\mu)^2$ which are

```{r}
sum(sampleNorm)/length(sampleNorm)
sum((sampleNorm - mean(sampleNorm))^2)/length(sampleNorm)
```

# Assignments


Replicate these steps for the Gamma Distribution, and derive the Inverse Gamma Distribution.
